{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# After Training\n",
        "\n",
        "\n",
        "In this activity, you will create a deep learning model from the credit score data, save it, and load it to evaluate its performance on unseen data.\n",
        "\n",
        "1. Split the data into training and test sets using the `train_test_split` method from `sklearn`. Then scale the features data using an instance of the `StandardScaler`.\n",
        "\n",
        "2. Using the training set, construct a shallow neural net model to predict the credit score features (you can use the same model that you constructed in the _Credit Scoring_ Activity).\n",
        "\n",
        "> **Note** When fitting the model, you will not need a `validation-split` parameter because the data was seperated into training and testing datasets.\n",
        "\n",
        "3. Using relative file paths, save the model and its weights.\n",
        "\n",
        "4. Load the model and its weights.\n",
        "\n",
        "5.  Use this loaded model to predict points for the test data and print the mean squared error metric for the predicted points vs. the actual points.\n",
        "\n",
        "## The Dataset\n",
        "\n",
        "This dataset is built around the same dataset used in the previous activity. The dataset contains `68` encoded features (columns from `0` to `67`), with all personal identifying information removed. The last two columns of the dataset (columns `68` and `69`) are preliminary credit score quality indicators that have been manually assigned by staff at the firm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.161286</td>\n",
              "      <td>7.835325</td>\n",
              "      <td>2.911583</td>\n",
              "      <td>0.984049</td>\n",
              "      <td>-1.499546</td>\n",
              "      <td>-2.094097</td>\n",
              "      <td>0.576000</td>\n",
              "      <td>-1.205671</td>\n",
              "      <td>1.849122</td>\n",
              "      <td>-0.425598</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.504263</td>\n",
              "      <td>0.351267</td>\n",
              "      <td>-1.018726</td>\n",
              "      <td>-0.174878</td>\n",
              "      <td>-1.089543</td>\n",
              "      <td>-0.668840</td>\n",
              "      <td>-0.914772</td>\n",
              "      <td>-0.836250</td>\n",
              "      <td>-15.75</td>\n",
              "      <td>-47.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.225763</td>\n",
              "      <td>-0.094169</td>\n",
              "      <td>-0.603646</td>\n",
              "      <td>0.497745</td>\n",
              "      <td>0.874036</td>\n",
              "      <td>0.290280</td>\n",
              "      <td>-0.077659</td>\n",
              "      <td>-0.887385</td>\n",
              "      <td>0.432062</td>\n",
              "      <td>-0.093963</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.495712</td>\n",
              "      <td>-0.465077</td>\n",
              "      <td>-0.157861</td>\n",
              "      <td>-0.157189</td>\n",
              "      <td>0.380951</td>\n",
              "      <td>1.088478</td>\n",
              "      <td>-0.123595</td>\n",
              "      <td>1.391141</td>\n",
              "      <td>14.91</td>\n",
              "      <td>-23.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.692525</td>\n",
              "      <td>-0.517801</td>\n",
              "      <td>-0.788035</td>\n",
              "      <td>1.214351</td>\n",
              "      <td>-0.907214</td>\n",
              "      <td>0.880213</td>\n",
              "      <td>0.406899</td>\n",
              "      <td>-0.694895</td>\n",
              "      <td>-0.901869</td>\n",
              "      <td>-1.701574</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.637167</td>\n",
              "      <td>0.147260</td>\n",
              "      <td>0.217914</td>\n",
              "      <td>2.718442</td>\n",
              "      <td>0.972919</td>\n",
              "      <td>2.081069</td>\n",
              "      <td>1.375763</td>\n",
              "      <td>1.063847</td>\n",
              "      <td>12.65</td>\n",
              "      <td>-8.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.735562</td>\n",
              "      <td>-0.684055</td>\n",
              "      <td>2.058215</td>\n",
              "      <td>0.716328</td>\n",
              "      <td>-0.011393</td>\n",
              "      <td>0.805396</td>\n",
              "      <td>1.497982</td>\n",
              "      <td>0.114752</td>\n",
              "      <td>0.692847</td>\n",
              "      <td>0.052377</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.178325</td>\n",
              "      <td>-0.065059</td>\n",
              "      <td>-0.724247</td>\n",
              "      <td>-1.020687</td>\n",
              "      <td>-0.751380</td>\n",
              "      <td>-0.385005</td>\n",
              "      <td>-0.012326</td>\n",
              "      <td>-0.392197</td>\n",
              "      <td>9.03</td>\n",
              "      <td>38.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.570272</td>\n",
              "      <td>0.273157</td>\n",
              "      <td>-0.279214</td>\n",
              "      <td>0.083456</td>\n",
              "      <td>1.049331</td>\n",
              "      <td>-0.869295</td>\n",
              "      <td>-0.265858</td>\n",
              "      <td>-0.401676</td>\n",
              "      <td>-0.872639</td>\n",
              "      <td>1.147483</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.919463</td>\n",
              "      <td>-0.667912</td>\n",
              "      <td>-0.820172</td>\n",
              "      <td>-0.190488</td>\n",
              "      <td>0.306974</td>\n",
              "      <td>0.119658</td>\n",
              "      <td>0.271838</td>\n",
              "      <td>1.289783</td>\n",
              "      <td>34.03</td>\n",
              "      <td>-6.85</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 70 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2         3         4         5         6   \\\n",
              "0  7.161286  7.835325  2.911583  0.984049 -1.499546 -2.094097  0.576000   \n",
              "1  0.225763 -0.094169 -0.603646  0.497745  0.874036  0.290280 -0.077659   \n",
              "2 -0.692525 -0.517801 -0.788035  1.214351 -0.907214  0.880213  0.406899   \n",
              "3 -0.735562 -0.684055  2.058215  0.716328 -0.011393  0.805396  1.497982   \n",
              "4  0.570272  0.273157 -0.279214  0.083456  1.049331 -0.869295 -0.265858   \n",
              "\n",
              "         7         8         9   ...        60        61        62        63  \\\n",
              "0 -1.205671  1.849122 -0.425598  ... -1.504263  0.351267 -1.018726 -0.174878   \n",
              "1 -0.887385  0.432062 -0.093963  ... -0.495712 -0.465077 -0.157861 -0.157189   \n",
              "2 -0.694895 -0.901869 -1.701574  ... -0.637167  0.147260  0.217914  2.718442   \n",
              "3  0.114752  0.692847  0.052377  ... -0.178325 -0.065059 -0.724247 -1.020687   \n",
              "4 -0.401676 -0.872639  1.147483  ... -0.919463 -0.667912 -0.820172 -0.190488   \n",
              "\n",
              "         64        65        66        67     68     69  \n",
              "0 -1.089543 -0.668840 -0.914772 -0.836250 -15.75 -47.95  \n",
              "1  0.380951  1.088478 -0.123595  1.391141  14.91 -23.51  \n",
              "2  0.972919  2.081069  1.375763  1.063847  12.65  -8.00  \n",
              "3 -0.751380 -0.385005 -0.012326 -0.392197   9.03  38.74  \n",
              "4  0.306974  0.119658  0.271838  1.289783  34.03  -6.85  \n",
              "\n",
              "[5 rows x 70 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read in data\n",
        "data = Path(\"../Resources/credit_scores.csv\")\n",
        "df = pd.read_csv(data, header=None)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.161286</td>\n",
              "      <td>7.835325</td>\n",
              "      <td>2.911583</td>\n",
              "      <td>0.984049</td>\n",
              "      <td>-1.499546</td>\n",
              "      <td>-2.094097</td>\n",
              "      <td>0.576000</td>\n",
              "      <td>-1.205671</td>\n",
              "      <td>1.849122</td>\n",
              "      <td>-0.425598</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.944584</td>\n",
              "      <td>-0.043610</td>\n",
              "      <td>-1.504263</td>\n",
              "      <td>0.351267</td>\n",
              "      <td>-1.018726</td>\n",
              "      <td>-0.174878</td>\n",
              "      <td>-1.089543</td>\n",
              "      <td>-0.668840</td>\n",
              "      <td>-0.914772</td>\n",
              "      <td>-0.836250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.225763</td>\n",
              "      <td>-0.094169</td>\n",
              "      <td>-0.603646</td>\n",
              "      <td>0.497745</td>\n",
              "      <td>0.874036</td>\n",
              "      <td>0.290280</td>\n",
              "      <td>-0.077659</td>\n",
              "      <td>-0.887385</td>\n",
              "      <td>0.432062</td>\n",
              "      <td>-0.093963</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.082645</td>\n",
              "      <td>-0.947933</td>\n",
              "      <td>-0.495712</td>\n",
              "      <td>-0.465077</td>\n",
              "      <td>-0.157861</td>\n",
              "      <td>-0.157189</td>\n",
              "      <td>0.380951</td>\n",
              "      <td>1.088478</td>\n",
              "      <td>-0.123595</td>\n",
              "      <td>1.391141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.692525</td>\n",
              "      <td>-0.517801</td>\n",
              "      <td>-0.788035</td>\n",
              "      <td>1.214351</td>\n",
              "      <td>-0.907214</td>\n",
              "      <td>0.880213</td>\n",
              "      <td>0.406899</td>\n",
              "      <td>-0.694895</td>\n",
              "      <td>-0.901869</td>\n",
              "      <td>-1.701574</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.797954</td>\n",
              "      <td>-0.556109</td>\n",
              "      <td>-0.637167</td>\n",
              "      <td>0.147260</td>\n",
              "      <td>0.217914</td>\n",
              "      <td>2.718442</td>\n",
              "      <td>0.972919</td>\n",
              "      <td>2.081069</td>\n",
              "      <td>1.375763</td>\n",
              "      <td>1.063847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.735562</td>\n",
              "      <td>-0.684055</td>\n",
              "      <td>2.058215</td>\n",
              "      <td>0.716328</td>\n",
              "      <td>-0.011393</td>\n",
              "      <td>0.805396</td>\n",
              "      <td>1.497982</td>\n",
              "      <td>0.114752</td>\n",
              "      <td>0.692847</td>\n",
              "      <td>0.052377</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.805626</td>\n",
              "      <td>0.166616</td>\n",
              "      <td>-0.178325</td>\n",
              "      <td>-0.065059</td>\n",
              "      <td>-0.724247</td>\n",
              "      <td>-1.020687</td>\n",
              "      <td>-0.751380</td>\n",
              "      <td>-0.385005</td>\n",
              "      <td>-0.012326</td>\n",
              "      <td>-0.392197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.570272</td>\n",
              "      <td>0.273157</td>\n",
              "      <td>-0.279214</td>\n",
              "      <td>0.083456</td>\n",
              "      <td>1.049331</td>\n",
              "      <td>-0.869295</td>\n",
              "      <td>-0.265858</td>\n",
              "      <td>-0.401676</td>\n",
              "      <td>-0.872639</td>\n",
              "      <td>1.147483</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.180181</td>\n",
              "      <td>-0.500785</td>\n",
              "      <td>-0.919463</td>\n",
              "      <td>-0.667912</td>\n",
              "      <td>-0.820172</td>\n",
              "      <td>-0.190488</td>\n",
              "      <td>0.306974</td>\n",
              "      <td>0.119658</td>\n",
              "      <td>0.271838</td>\n",
              "      <td>1.289783</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 68 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2         3         4         5         6   \\\n",
              "0  7.161286  7.835325  2.911583  0.984049 -1.499546 -2.094097  0.576000   \n",
              "1  0.225763 -0.094169 -0.603646  0.497745  0.874036  0.290280 -0.077659   \n",
              "2 -0.692525 -0.517801 -0.788035  1.214351 -0.907214  0.880213  0.406899   \n",
              "3 -0.735562 -0.684055  2.058215  0.716328 -0.011393  0.805396  1.497982   \n",
              "4  0.570272  0.273157 -0.279214  0.083456  1.049331 -0.869295 -0.265858   \n",
              "\n",
              "         7         8         9   ...        58        59        60        61  \\\n",
              "0 -1.205671  1.849122 -0.425598  ... -0.944584 -0.043610 -1.504263  0.351267   \n",
              "1 -0.887385  0.432062 -0.093963  ... -0.082645 -0.947933 -0.495712 -0.465077   \n",
              "2 -0.694895 -0.901869 -1.701574  ... -0.797954 -0.556109 -0.637167  0.147260   \n",
              "3  0.114752  0.692847  0.052377  ... -0.805626  0.166616 -0.178325 -0.065059   \n",
              "4 -0.401676 -0.872639  1.147483  ... -0.180181 -0.500785 -0.919463 -0.667912   \n",
              "\n",
              "         62        63        64        65        66        67  \n",
              "0 -1.018726 -0.174878 -1.089543 -0.668840 -0.914772 -0.836250  \n",
              "1 -0.157861 -0.157189  0.380951  1.088478 -0.123595  1.391141  \n",
              "2  0.217914  2.718442  0.972919  2.081069  1.375763  1.063847  \n",
              "3 -0.724247 -1.020687 -0.751380 -0.385005 -0.012326 -0.392197  \n",
              "4 -0.820172 -0.190488  0.306974  0.119658  0.271838  1.289783  \n",
              "\n",
              "[5 rows x 68 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the features set 'X', and the target 'y' set\n",
        "\n",
        "# The features dataset consists of columns 0 to 67\n",
        "X = df.iloc[:, 0:68]\n",
        "\n",
        "# The target conststs of columns 68 and 69\n",
        "y = df.iloc[:, 68:70]\n",
        "\n",
        "# View data for the features set\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Split the data into training and test sets using the `train_test_split` method from `sklearn`. Then scale the features data using an instance of the `StandardScaler`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into traning and testing sets using the train_test_split \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale the data for the features set X_tain and X_test\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the training data to a StandardScaler instance\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# Scale the training data\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "\n",
        "# Scale the testing data\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Using the training set, construct a shallow neural net model to predict the credit score data (you can use the same model that you constructed in the _Credit Scoring_ Activity)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a shallow, 1 hidden layer, neural network\n",
        "number_input_features = 68\n",
        "hidden_nodes_layer1 = 2\n",
        "\n",
        "# Instantiate an instance of the Sequential model\n",
        "nn_1 = Sequential()\n",
        "\n",
        "# Add the first hidden layer\n",
        "nn_1.add(Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))\n",
        "\n",
        "# Add the output layer\n",
        "nn_1.add(Dense(units=1, activation=\"linear\"))\n",
        "\n",
        "# Compile the model \n",
        "# Set the parameters as mean_squared_error, adam, and mse.\n",
        "nn_1.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mse\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "25/25 [==============================] - 1s 3ms/step - loss: 2440.5044 - mse: 2440.5044\n",
            "Epoch 2/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 2423.7576 - mse: 2423.7576\n",
            "Epoch 3/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 2403.6182 - mse: 2403.6182\n",
            "Epoch 4/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 2380.3208 - mse: 2380.3208\n",
            "Epoch 5/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 2355.2773 - mse: 2355.2773\n",
            "Epoch 6/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 2327.2305 - mse: 2327.2305\n",
            "Epoch 7/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 2296.8582 - mse: 2296.8582\n",
            "Epoch 8/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 2265.5715 - mse: 2265.5715\n",
            "Epoch 9/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 2232.3545 - mse: 2232.3545\n",
            "Epoch 10/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 2198.8862 - mse: 2198.8862\n",
            "Epoch 11/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 2163.4146 - mse: 2163.4146\n",
            "Epoch 12/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 2127.7639 - mse: 2127.7639\n",
            "Epoch 13/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 2091.8992 - mse: 2091.8992\n",
            "Epoch 14/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 2055.2246 - mse: 2055.2246\n",
            "Epoch 15/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 2019.0767 - mse: 2019.0767\n",
            "Epoch 16/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1983.8354 - mse: 1983.8354\n",
            "Epoch 17/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1948.3185 - mse: 1948.3185\n",
            "Epoch 18/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1914.1696 - mse: 1914.1696\n",
            "Epoch 19/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1881.6359 - mse: 1881.6359\n",
            "Epoch 20/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1850.1163 - mse: 1850.1163\n",
            "Epoch 21/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1819.7822 - mse: 1819.7822\n",
            "Epoch 22/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1791.2504 - mse: 1791.2504\n",
            "Epoch 23/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1764.9021 - mse: 1764.9021\n",
            "Epoch 24/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1739.2390 - mse: 1739.2390\n",
            "Epoch 25/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1716.2294 - mse: 1716.2294\n",
            "Epoch 26/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1694.0243 - mse: 1694.0243\n",
            "Epoch 27/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1674.0775 - mse: 1674.0775\n",
            "Epoch 28/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1655.0696 - mse: 1655.0696\n",
            "Epoch 29/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1638.3707 - mse: 1638.3707\n",
            "Epoch 30/50\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 1622.6743 - mse: 1622.6743\n",
            "Epoch 31/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1608.4316 - mse: 1608.4316\n",
            "Epoch 32/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1595.5730 - mse: 1595.5730\n",
            "Epoch 33/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1583.8806 - mse: 1583.8806\n",
            "Epoch 34/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1573.5115 - mse: 1573.5115\n",
            "Epoch 35/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1563.6281 - mse: 1563.6281\n",
            "Epoch 36/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1555.1838 - mse: 1555.1838\n",
            "Epoch 37/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 1547.0183 - mse: 1547.0183\n",
            "Epoch 38/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 1539.5962 - mse: 1539.5962\n",
            "Epoch 39/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 1533.0559 - mse: 1533.0559\n",
            "Epoch 40/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1526.8263 - mse: 1526.8263\n",
            "Epoch 41/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1520.7433 - mse: 1520.7433\n",
            "Epoch 42/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1515.5629 - mse: 1515.5629\n",
            "Epoch 43/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1510.5521 - mse: 1510.5521\n",
            "Epoch 44/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1506.0460 - mse: 1506.0460\n",
            "Epoch 45/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1501.7178 - mse: 1501.7178\n",
            "Epoch 46/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1497.6870 - mse: 1497.6870\n",
            "Epoch 47/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1493.9940 - mse: 1493.9940\n",
            "Epoch 48/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1490.2982 - mse: 1490.2982\n",
            "Epoch 49/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1486.7310 - mse: 1486.7310\n",
            "Epoch 50/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1483.3596 - mse: 1483.3596\n"
          ]
        }
      ],
      "source": [
        "# Fit the model using the training data\n",
        "deep_net_model = nn_1.fit(X_train_scaled, y_train, epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the train function\n",
        "plt.plot(deep_net_model.history[\"loss\"])\n",
        "plt.title(\"loss_function - training\")\n",
        "plt.legend([\"loss\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Using relative file paths, save the model and its weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model in JSON format\n",
        "json_model = nn_1.to_json()\n",
        "\n",
        "# Define a relative path to save the model\n",
        "# The model should be saved with a .json file extension\n",
        "with open('../Resources/credit_scores_model.json', 'w') as json_file:\n",
        "    json_file.write(json_model)\n",
        "\n",
        "# Define a relative path to save the model weights\n",
        "# The model weights should be saved with a .h5 file extension\n",
        "nn_1.save_weights('../Resources/credit_scores_model.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Load the model and its weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the model to predict values\n",
        "import tensorflow as tf\n",
        "\n",
        "# Identify the relative path of the model's location\n",
        "with open('../Resources/credit_scores_model.json', 'r') as json_file:\n",
        "    json_savedModel= json_file.read()\n",
        "\n",
        "model_j = tf.keras.models.model_from_json(json_savedModel)\n",
        "\n",
        "# Read in the model\n",
        "#credit_scores_model_imported = tf.keras.models.load_model(file_path)\n",
        "\n",
        "# Identify the relative path for the model's weights\n",
        "\n",
        "# Load the model's weights \n",
        "model_j.load_weights('../Resources/credit_scores_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Use this loaded model to predict points for the test data and print the mean squared error metric for the predicted points vs. the actual points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[10.047198 ],\n",
              "       [61.294426 ],\n",
              "       [31.026781 ],\n",
              "       [ 5.1989865],\n",
              "       [18.547438 ]], dtype=float32)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Predict values using the testing data\n",
        "y_pred = model_j.predict(X_test)\n",
        "\n",
        "# View the model's predictions\n",
        "y_pred[:5, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9/9 - 1s - loss: 1698.8198 - mse: 1698.8198 - 532ms/epoch - 59ms/step\n",
            "Loss: 1698.81982421875, Accuracy: 1698.81982421875\n"
          ]
        }
      ],
      "source": [
        "# Import\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "model_j.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mse\"])\n",
        "\n",
        "# Evaluate the model with the MSE metric\n",
        "model_loss, model_accuracy = model_j.evaluate(X_test_scaled, y_test, verbose=2)\n",
        "\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3.7.13 ('dev')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "nteract": {
      "version": "0.28.0"
    },
    "pygments_lexer": "ipython3",
    "version": 3,
    "vscode": {
      "interpreter": {
        "hash": "6a7ae136a53a24601e63a177d18d4029f938b6714f24b844ae865dcffb9f6766"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
