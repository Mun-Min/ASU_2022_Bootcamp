{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# After Training\n",
        "\n",
        "\n",
        "In this activity, you will create a deep learning model from the credit score data, save it, and load it to evaluate its performance on unseen data.\n",
        "\n",
        "1. Split the data into training and test sets using the `train_test_split` method from `sklearn`. Then scale the features data using an instance of the `StandardScaler`.\n",
        "\n",
        "2. Using the training set, construct a shallow neural net model to predict the credit score features (you can use the same model that you constructed in the _Credit Scoring_ Activity).\n",
        "\n",
        "> **Note** When fitting the model, you will not need a `validation-split` parameter because the data was seperated into training and testing datasets.\n",
        "\n",
        "3. Using relative file paths, save the model and its weights.\n",
        "\n",
        "4. Load the model and its weights.\n",
        "\n",
        "5.  Use this loaded model to predict points for the test data and print the mean squared error metric for the predicted points vs. the actual points.\n",
        "\n",
        "## The Dataset\n",
        "\n",
        "This dataset is built around the same dataset used in the previous activity. The dataset contains `68` encoded features (columns from `0` to `67`), with all personal identifying information removed. The last two columns of the dataset (columns `68` and `69`) are preliminary credit score quality indicators that have been manually assigned by staff at the firm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.161286</td>\n",
              "      <td>7.835325</td>\n",
              "      <td>2.911583</td>\n",
              "      <td>0.984049</td>\n",
              "      <td>-1.499546</td>\n",
              "      <td>-2.094097</td>\n",
              "      <td>0.576000</td>\n",
              "      <td>-1.205671</td>\n",
              "      <td>1.849122</td>\n",
              "      <td>-0.425598</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.504263</td>\n",
              "      <td>0.351267</td>\n",
              "      <td>-1.018726</td>\n",
              "      <td>-0.174878</td>\n",
              "      <td>-1.089543</td>\n",
              "      <td>-0.668840</td>\n",
              "      <td>-0.914772</td>\n",
              "      <td>-0.836250</td>\n",
              "      <td>-15.75</td>\n",
              "      <td>-47.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.225763</td>\n",
              "      <td>-0.094169</td>\n",
              "      <td>-0.603646</td>\n",
              "      <td>0.497745</td>\n",
              "      <td>0.874036</td>\n",
              "      <td>0.290280</td>\n",
              "      <td>-0.077659</td>\n",
              "      <td>-0.887385</td>\n",
              "      <td>0.432062</td>\n",
              "      <td>-0.093963</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.495712</td>\n",
              "      <td>-0.465077</td>\n",
              "      <td>-0.157861</td>\n",
              "      <td>-0.157189</td>\n",
              "      <td>0.380951</td>\n",
              "      <td>1.088478</td>\n",
              "      <td>-0.123595</td>\n",
              "      <td>1.391141</td>\n",
              "      <td>14.91</td>\n",
              "      <td>-23.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.692525</td>\n",
              "      <td>-0.517801</td>\n",
              "      <td>-0.788035</td>\n",
              "      <td>1.214351</td>\n",
              "      <td>-0.907214</td>\n",
              "      <td>0.880213</td>\n",
              "      <td>0.406899</td>\n",
              "      <td>-0.694895</td>\n",
              "      <td>-0.901869</td>\n",
              "      <td>-1.701574</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.637167</td>\n",
              "      <td>0.147260</td>\n",
              "      <td>0.217914</td>\n",
              "      <td>2.718442</td>\n",
              "      <td>0.972919</td>\n",
              "      <td>2.081069</td>\n",
              "      <td>1.375763</td>\n",
              "      <td>1.063847</td>\n",
              "      <td>12.65</td>\n",
              "      <td>-8.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.735562</td>\n",
              "      <td>-0.684055</td>\n",
              "      <td>2.058215</td>\n",
              "      <td>0.716328</td>\n",
              "      <td>-0.011393</td>\n",
              "      <td>0.805396</td>\n",
              "      <td>1.497982</td>\n",
              "      <td>0.114752</td>\n",
              "      <td>0.692847</td>\n",
              "      <td>0.052377</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.178325</td>\n",
              "      <td>-0.065059</td>\n",
              "      <td>-0.724247</td>\n",
              "      <td>-1.020687</td>\n",
              "      <td>-0.751380</td>\n",
              "      <td>-0.385005</td>\n",
              "      <td>-0.012326</td>\n",
              "      <td>-0.392197</td>\n",
              "      <td>9.03</td>\n",
              "      <td>38.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.570272</td>\n",
              "      <td>0.273157</td>\n",
              "      <td>-0.279214</td>\n",
              "      <td>0.083456</td>\n",
              "      <td>1.049331</td>\n",
              "      <td>-0.869295</td>\n",
              "      <td>-0.265858</td>\n",
              "      <td>-0.401676</td>\n",
              "      <td>-0.872639</td>\n",
              "      <td>1.147483</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.919463</td>\n",
              "      <td>-0.667912</td>\n",
              "      <td>-0.820172</td>\n",
              "      <td>-0.190488</td>\n",
              "      <td>0.306974</td>\n",
              "      <td>0.119658</td>\n",
              "      <td>0.271838</td>\n",
              "      <td>1.289783</td>\n",
              "      <td>34.03</td>\n",
              "      <td>-6.85</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 70 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2         3         4         5         6   \\\n",
              "0  7.161286  7.835325  2.911583  0.984049 -1.499546 -2.094097  0.576000   \n",
              "1  0.225763 -0.094169 -0.603646  0.497745  0.874036  0.290280 -0.077659   \n",
              "2 -0.692525 -0.517801 -0.788035  1.214351 -0.907214  0.880213  0.406899   \n",
              "3 -0.735562 -0.684055  2.058215  0.716328 -0.011393  0.805396  1.497982   \n",
              "4  0.570272  0.273157 -0.279214  0.083456  1.049331 -0.869295 -0.265858   \n",
              "\n",
              "         7         8         9   ...        60        61        62        63  \\\n",
              "0 -1.205671  1.849122 -0.425598  ... -1.504263  0.351267 -1.018726 -0.174878   \n",
              "1 -0.887385  0.432062 -0.093963  ... -0.495712 -0.465077 -0.157861 -0.157189   \n",
              "2 -0.694895 -0.901869 -1.701574  ... -0.637167  0.147260  0.217914  2.718442   \n",
              "3  0.114752  0.692847  0.052377  ... -0.178325 -0.065059 -0.724247 -1.020687   \n",
              "4 -0.401676 -0.872639  1.147483  ... -0.919463 -0.667912 -0.820172 -0.190488   \n",
              "\n",
              "         64        65        66        67     68     69  \n",
              "0 -1.089543 -0.668840 -0.914772 -0.836250 -15.75 -47.95  \n",
              "1  0.380951  1.088478 -0.123595  1.391141  14.91 -23.51  \n",
              "2  0.972919  2.081069  1.375763  1.063847  12.65  -8.00  \n",
              "3 -0.751380 -0.385005 -0.012326 -0.392197   9.03  38.74  \n",
              "4  0.306974  0.119658  0.271838  1.289783  34.03  -6.85  \n",
              "\n",
              "[5 rows x 70 columns]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read in data\n",
        "data = Path(\"../Resources/credit_scores.csv\")\n",
        "df = pd.read_csv(data, header=None)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.161286</td>\n",
              "      <td>7.835325</td>\n",
              "      <td>2.911583</td>\n",
              "      <td>0.984049</td>\n",
              "      <td>-1.499546</td>\n",
              "      <td>-2.094097</td>\n",
              "      <td>0.576000</td>\n",
              "      <td>-1.205671</td>\n",
              "      <td>1.849122</td>\n",
              "      <td>-0.425598</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.944584</td>\n",
              "      <td>-0.043610</td>\n",
              "      <td>-1.504263</td>\n",
              "      <td>0.351267</td>\n",
              "      <td>-1.018726</td>\n",
              "      <td>-0.174878</td>\n",
              "      <td>-1.089543</td>\n",
              "      <td>-0.668840</td>\n",
              "      <td>-0.914772</td>\n",
              "      <td>-0.836250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.225763</td>\n",
              "      <td>-0.094169</td>\n",
              "      <td>-0.603646</td>\n",
              "      <td>0.497745</td>\n",
              "      <td>0.874036</td>\n",
              "      <td>0.290280</td>\n",
              "      <td>-0.077659</td>\n",
              "      <td>-0.887385</td>\n",
              "      <td>0.432062</td>\n",
              "      <td>-0.093963</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.082645</td>\n",
              "      <td>-0.947933</td>\n",
              "      <td>-0.495712</td>\n",
              "      <td>-0.465077</td>\n",
              "      <td>-0.157861</td>\n",
              "      <td>-0.157189</td>\n",
              "      <td>0.380951</td>\n",
              "      <td>1.088478</td>\n",
              "      <td>-0.123595</td>\n",
              "      <td>1.391141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.692525</td>\n",
              "      <td>-0.517801</td>\n",
              "      <td>-0.788035</td>\n",
              "      <td>1.214351</td>\n",
              "      <td>-0.907214</td>\n",
              "      <td>0.880213</td>\n",
              "      <td>0.406899</td>\n",
              "      <td>-0.694895</td>\n",
              "      <td>-0.901869</td>\n",
              "      <td>-1.701574</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.797954</td>\n",
              "      <td>-0.556109</td>\n",
              "      <td>-0.637167</td>\n",
              "      <td>0.147260</td>\n",
              "      <td>0.217914</td>\n",
              "      <td>2.718442</td>\n",
              "      <td>0.972919</td>\n",
              "      <td>2.081069</td>\n",
              "      <td>1.375763</td>\n",
              "      <td>1.063847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.735562</td>\n",
              "      <td>-0.684055</td>\n",
              "      <td>2.058215</td>\n",
              "      <td>0.716328</td>\n",
              "      <td>-0.011393</td>\n",
              "      <td>0.805396</td>\n",
              "      <td>1.497982</td>\n",
              "      <td>0.114752</td>\n",
              "      <td>0.692847</td>\n",
              "      <td>0.052377</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.805626</td>\n",
              "      <td>0.166616</td>\n",
              "      <td>-0.178325</td>\n",
              "      <td>-0.065059</td>\n",
              "      <td>-0.724247</td>\n",
              "      <td>-1.020687</td>\n",
              "      <td>-0.751380</td>\n",
              "      <td>-0.385005</td>\n",
              "      <td>-0.012326</td>\n",
              "      <td>-0.392197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.570272</td>\n",
              "      <td>0.273157</td>\n",
              "      <td>-0.279214</td>\n",
              "      <td>0.083456</td>\n",
              "      <td>1.049331</td>\n",
              "      <td>-0.869295</td>\n",
              "      <td>-0.265858</td>\n",
              "      <td>-0.401676</td>\n",
              "      <td>-0.872639</td>\n",
              "      <td>1.147483</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.180181</td>\n",
              "      <td>-0.500785</td>\n",
              "      <td>-0.919463</td>\n",
              "      <td>-0.667912</td>\n",
              "      <td>-0.820172</td>\n",
              "      <td>-0.190488</td>\n",
              "      <td>0.306974</td>\n",
              "      <td>0.119658</td>\n",
              "      <td>0.271838</td>\n",
              "      <td>1.289783</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 68 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2         3         4         5         6   \\\n",
              "0  7.161286  7.835325  2.911583  0.984049 -1.499546 -2.094097  0.576000   \n",
              "1  0.225763 -0.094169 -0.603646  0.497745  0.874036  0.290280 -0.077659   \n",
              "2 -0.692525 -0.517801 -0.788035  1.214351 -0.907214  0.880213  0.406899   \n",
              "3 -0.735562 -0.684055  2.058215  0.716328 -0.011393  0.805396  1.497982   \n",
              "4  0.570272  0.273157 -0.279214  0.083456  1.049331 -0.869295 -0.265858   \n",
              "\n",
              "         7         8         9   ...        58        59        60        61  \\\n",
              "0 -1.205671  1.849122 -0.425598  ... -0.944584 -0.043610 -1.504263  0.351267   \n",
              "1 -0.887385  0.432062 -0.093963  ... -0.082645 -0.947933 -0.495712 -0.465077   \n",
              "2 -0.694895 -0.901869 -1.701574  ... -0.797954 -0.556109 -0.637167  0.147260   \n",
              "3  0.114752  0.692847  0.052377  ... -0.805626  0.166616 -0.178325 -0.065059   \n",
              "4 -0.401676 -0.872639  1.147483  ... -0.180181 -0.500785 -0.919463 -0.667912   \n",
              "\n",
              "         62        63        64        65        66        67  \n",
              "0 -1.018726 -0.174878 -1.089543 -0.668840 -0.914772 -0.836250  \n",
              "1 -0.157861 -0.157189  0.380951  1.088478 -0.123595  1.391141  \n",
              "2  0.217914  2.718442  0.972919  2.081069  1.375763  1.063847  \n",
              "3 -0.724247 -1.020687 -0.751380 -0.385005 -0.012326 -0.392197  \n",
              "4 -0.820172 -0.190488  0.306974  0.119658  0.271838  1.289783  \n",
              "\n",
              "[5 rows x 68 columns]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the features set 'X', and the target 'y' set\n",
        "\n",
        "# The features dataset consists of columns 0 to 67\n",
        "X = df.iloc[:, 0:68]\n",
        "\n",
        "# The target conststs of columns 68 and 69\n",
        "y = df.iloc[:, 68:70]\n",
        "\n",
        "# View data for the features set\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Split the data into training and test sets using the `train_test_split` method from `sklearn`. Then scale the features data using an instance of the `StandardScaler`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into traning and testing sets using the train_test_split \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale the data for the features set X_tain and X_test\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the training data to a StandardScaler instance\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# Scale the training data\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "\n",
        "# Scale the testing data\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Using the training set, construct a shallow neural net model to predict the credit score data (you can use the same model that you constructed in the _Credit Scoring_ Activity)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a shallow, 1 hidden layer, neural network\n",
        "number_input_features = 68\n",
        "hidden_nodes_layer1 = 2\n",
        "\n",
        "# Instantiate an instance of the Sequential model\n",
        "nn_1 = Sequential()\n",
        "\n",
        "# Add the first hidden layer\n",
        "nn_1.add(Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))\n",
        "\n",
        "# Add the output layer\n",
        "nn_1.add(Dense(units=1, activation=\"linear\"))\n",
        "\n",
        "# Compile the model \n",
        "# Set the parameters as mean_squared_error, adam, and mse.\n",
        "nn_1.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mse\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2531.2874 - mse: 2531.2874\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2530.1770 - mse: 2530.1770\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2529.0730 - mse: 2529.0730\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2527.9756 - mse: 2527.9756\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2526.8845 - mse: 2526.8845\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2525.7976 - mse: 2525.7976\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2524.7180 - mse: 2524.7180\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2523.6628 - mse: 2523.6628\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2522.6274 - mse: 2522.6274\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2521.6072 - mse: 2521.6072\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 2520.5911 - mse: 2520.5911\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2519.5762 - mse: 2519.5762\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2518.5669 - mse: 2518.5669\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2517.5605 - mse: 2517.5605\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2516.5610 - mse: 2516.5610\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2515.5654 - mse: 2515.5654\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2514.5710 - mse: 2514.5710\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2513.5815 - mse: 2513.5815\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2512.6047 - mse: 2512.6047\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2511.6384 - mse: 2511.6384\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2510.6777 - mse: 2510.6777\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2509.7268 - mse: 2509.7268\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2508.7903 - mse: 2508.7903\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2507.8574 - mse: 2507.8574\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2506.9377 - mse: 2506.9377\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2506.0422 - mse: 2506.0422\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2505.1604 - mse: 2505.1604\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2504.2800 - mse: 2504.2800\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2503.4163 - mse: 2503.4163\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2502.5681 - mse: 2502.5681\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2501.7271 - mse: 2501.7271\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2500.8948 - mse: 2500.8948\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2500.0632 - mse: 2500.0632\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2499.2344 - mse: 2499.2344\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2498.4048 - mse: 2498.4048\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2497.5730 - mse: 2497.5730\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2496.7476 - mse: 2496.7476\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2495.9263 - mse: 2495.9263\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2495.1150 - mse: 2495.1150\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2494.3125 - mse: 2494.3125\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2493.5090 - mse: 2493.5090\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2492.7063 - mse: 2492.7063\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2491.9050 - mse: 2491.9050\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2491.1021 - mse: 2491.1021\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2490.2971 - mse: 2490.2971\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2489.4922 - mse: 2489.4922\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2488.6868 - mse: 2488.6868\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2487.8796 - mse: 2487.8796\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2487.0725 - mse: 2487.0725\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2486.2507 - mse: 2486.2507\n"
          ]
        }
      ],
      "source": [
        "# Fit the model using the training data\n",
        "deep_net_model = nn_1.fit(X_train_scaled, y_train, epochs=50, batch_size = 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the train function\n",
        "plt.plot(deep_net_model.history[\"loss\"])\n",
        "plt.title(\"loss_function - training\")\n",
        "plt.legend([\"loss\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Using relative file paths, save the model and its weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model in JSON format\n",
        "json_model = nn_1.to_json()\n",
        "\n",
        "# Define a relative path to save the model\n",
        "# The model should be saved with a .json file extension\n",
        "with open('../Resources/credit_scores_model.json', 'w') as json_file:\n",
        "    json_file.write(json_model)\n",
        "\n",
        "# Define a relative path to save the model weights\n",
        "# The model weights should be saved with a .h5 file extension\n",
        "nn_1.save_weights('../Resources/credit_scores_model.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Load the model and its weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the model to predict values\n",
        "import tensorflow as tf\n",
        "\n",
        "# Identify the relative path of the model's location\n",
        "with open('../Resources/credit_scores_model.json', 'r') as json_file:\n",
        "    json_savedModel= json_file.read()\n",
        "\n",
        "model_j = tf.keras.models.model_from_json(json_savedModel)\n",
        "\n",
        "# Read in the model\n",
        "#credit_scores_model_imported = tf.keras.models.load_model(file_path)\n",
        "\n",
        "# Identify the relative path for the model's weights\n",
        "\n",
        "# Load the model's weights \n",
        "model_j.load_weights('../Resources/credit_scores_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Use this loaded model to predict points for the test data and print the mean squared error metric for the predicted points vs. the actual points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - 0s 5ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[ 0.04990241],\n",
              "       [ 0.90681434],\n",
              "       [ 0.78712094],\n",
              "       [-0.34174985],\n",
              "       [ 0.23791143]], dtype=float32)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Predict values using the testing data\n",
        "y_pred = model_j.predict(X_test)\n",
        "\n",
        "# View the model's predictions\n",
        "y_pred[:5, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9/9 - 1s - loss: 2604.5750 - mse: 2604.5750 - 658ms/epoch - 73ms/step\n",
            "Loss: 2604.574951171875, Accuracy: 2604.574951171875\n"
          ]
        }
      ],
      "source": [
        "# Import\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "model_j.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mse\"])\n",
        "\n",
        "# Evaluate the model with the MSE metric\n",
        "model_loss, model_accuracy = model_j.evaluate(X_test_scaled, y_test, verbose=2)\n",
        "\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3.7.13 ('dev')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "nteract": {
      "version": "0.28.0"
    },
    "pygments_lexer": "ipython3",
    "version": 3,
    "vscode": {
      "interpreter": {
        "hash": "6a7ae136a53a24601e63a177d18d4029f938b6714f24b844ae865dcffb9f6766"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
